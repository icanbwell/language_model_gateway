[
  {
    "id": "language_model_gateway",
    "user_id": "6c03bf27-dbbf-44c1-b980-2c6a4608f712",
    "name": "language_model_gateway",
    "type": "pipe",
    "content": "\"\"\"title: LangChain Pipe Function (Streaming Version)\nauthor: Colby Sawyer @ Attollo LLC (mailto:colby.sawyer@attollodefense.com)\nauthor_url: https://github.com/ColbySawyer7\nversion: 0.2.0\nThis module defines a Pipe class that utilizes LangChain with streaming support\n\"\"\"\n\nimport asyncio\nimport json\nimport time\nfrom typing import AsyncGenerator\nfrom typing import Optional, Callable, Awaitable, Any, Dict, Union\n\nfrom pydantic import BaseModel, Field\nfrom starlette.requests import Request\nfrom starlette.responses import StreamingResponse\n\n\nclass Pipe:\n    class Valves(BaseModel):\n        emit_interval: float = Field(\n            default=2.0, description=\"Interval in seconds between status emissions\"\n        )\n        enable_status_indicator: bool = Field(\n            default=True, description=\"Enable or disable status indicator emissions\"\n        )\n\n    def __init__(self) -> None:\n        self.type: str = \"pipe\"\n        self.id: str = \"langchain_pipe\"\n        self.name: str = \"LangChain Pipe\"\n        self.valves = self.Valves()\n        self.last_emit_time: float = 0\n\n    async def emit_status(\n        self,\n        __event_emitter__: Optional[Callable[[Dict[str, Any]], Awaitable[None]]],\n        level: str,\n        message: str,\n        done: bool,\n    ) -> None:\n        current_time = time.time()\n        if (\n            __event_emitter__\n            and self.valves.enable_status_indicator\n            and (\n                current_time - self.last_emit_time >= self.valves.emit_interval or done\n            )\n        ):\n            await __event_emitter__(\n                {\n                    \"type\": \"status\",\n                    \"data\": {\n                        \"status\": \"complete\" if done else \"in_progress\",\n                        \"level\": level,\n                        \"description\": message,\n                        \"done\": done,\n                    },\n                }\n            )\n            self.last_emit_time = current_time\n\n    async def stream_response(\n        self,\n        *,\n        body: Dict[str, Any],\n        __request__: Optional[Request] = None,\n        __user__: Optional[Dict[str, Any]] = None,\n        __event_emitter__: Callable[[Dict[str, Any]], Awaitable[None]] | None = None,\n        __event_call__: Callable[[Dict[str, Any]], Awaitable[Dict[str, Any]]]\n        | None = None,\n    ) -> AsyncGenerator[str, None]:\n        \"\"\"\n        Async generator to stream response chunks\n        \"\"\"\n        try:\n            await self.emit_status(\n                __event_emitter__,\n                \"info\",\n                f\"/initiating Chain: headers={__request__.headers if __request__ else None}\"\n                f\", cookies={__request__.cookies if __request__ else None}\"\n                f\" {__user__=} {body=}\",\n                False,\n            )\n\n            if __request__ is None or __user__ is None:\n                raise ValueError(\"Request and user information must be provided.\")\n\n            # Simulate streaming response\n            # Generate chunks in OpenAI streaming format\n            chunks = [\n                {\n                    \"id\": \"chatcmpl-123\",\n                    \"object\": \"chat.completion.chunk\",\n                    \"created\": int(time.time()),\n                    \"model\": \"gpt-3.5-turbo\",\n                    \"choices\": [\n                        {\n                            \"index\": 0,\n                            \"delta\": {\"role\": \"assistant\"},\n                            \"finish_reason\": None,\n                        }\n                    ],\n                },\n                {\n                    \"id\": \"chatcmpl-123\",\n                    \"object\": \"chat.completion.chunk\",\n                    \"created\": int(time.time()),\n                    \"model\": \"gpt-3.5-turbo\",\n                    \"choices\": [\n                        {\n                            \"index\": 0,\n                            \"delta\": {\"content\": \"Here\"},\n                            \"finish_reason\": None,\n                        }\n                    ],\n                },\n                {\n                    \"id\": \"chatcmpl-123\",\n                    \"object\": \"chat.completion.chunk\",\n                    \"created\": int(time.time()),\n                    \"model\": \"gpt-3.5-turbo\",\n                    \"choices\": [\n                        {\"index\": 0, \"delta\": {\"content\": \" is\"}, \"finish_reason\": None}\n                    ],\n                },\n                {\n                    \"id\": \"chatcmpl-123\",\n                    \"object\": \"chat.completion.chunk\",\n                    \"created\": int(time.time()),\n                    \"model\": \"gpt-3.5-turbo\",\n                    \"choices\": [\n                        {\"index\": 0, \"delta\": {\"content\": \" a\"}, \"finish_reason\": None}\n                    ],\n                },\n                {\n                    \"id\": \"chatcmpl-123\",\n                    \"object\": \"chat.completion.chunk\",\n                    \"created\": int(time.time()),\n                    \"model\": \"gpt-3.5-turbo\",\n                    \"choices\": [\n                        {\n                            \"index\": 0,\n                            \"delta\": {\"content\": \" streamed\"},\n                            \"finish_reason\": None,\n                        }\n                    ],\n                },\n                {\n                    \"id\": \"chatcmpl-123\",\n                    \"object\": \"chat.completion.chunk\",\n                    \"created\": int(time.time()),\n                    \"model\": \"gpt-3.5-turbo\",\n                    \"choices\": [\n                        {\n                            \"index\": 0,\n                            \"delta\": {\n                                \"content\": f\"\\nheaders=\\n{__request__.headers}\\ncookies=\\n{__request__.cookies}\\n{__user__=}\\n{body=}\",\n                            },\n                            \"finish_reason\": None,\n                        }\n                    ],\n                },\n                {\n                    \"id\": \"chatcmpl-123\",\n                    \"object\": \"chat.completion.chunk\",\n                    \"created\": int(time.time()),\n                    \"model\": \"gpt-3.5-turbo\",\n                    \"choices\": [\n                        {\n                            \"index\": 0,\n                            \"delta\": {\n                                \"content\": f\"\\nOAuth_id_token:\\n{__request__.cookies.get('oauth_id_token')}\\n\",\n                            },\n                            \"finish_reason\": None,\n                        }\n                    ],\n                },\n                {\n                    \"id\": \"chatcmpl-123\",\n                    \"object\": \"chat.completion.chunk\",\n                    \"created\": int(time.time()),\n                    \"model\": \"gpt-3.5-turbo\",\n                    \"choices\": [{\"index\": 0, \"delta\": {}, \"finish_reason\": \"stop\"}],\n                },\n            ]\n\n            for chunk in chunks:\n                # Yield each chunk as a JSON-encoded string with a data: prefix\n                yield f\"data: {json.dumps(chunk)}\\n\\n\"\n                await self.emit_status(__event_emitter__, \"info\", \"Streaming...\", False)\n                await asyncio.sleep(0.5)  # Simulate streaming delay\n\n            await self.emit_status(__event_emitter__, \"info\", \"Stream Complete\", True)\n\n        except Exception as e:\n            error_chunk = {\n                \"id\": \"chatcmpl-error\",\n                \"object\": \"chat.completion.chunk\",\n                \"created\": int(time.time()),\n                \"model\": \"error\",\n                \"choices\": [\n                    {\n                        \"index\": 0,\n                        \"delta\": {\"content\": f\"Error: {str(e)}\"},\n                        \"finish_reason\": \"stop\",\n                    }\n                ],\n            }\n            yield f\"data: {json.dumps(error_chunk)}\\n\\n\"\n            await self.emit_status(__event_emitter__, \"error\", str(e), True)\n\n    async def pipe(\n        self,\n        body: Dict[str, Any],\n        __request__: Optional[Request] = None,\n        __user__: Optional[Dict[str, Any]] = None,\n        __event_emitter__: Optional[Callable[[Dict[str, Any]], Awaitable[None]]] = None,\n        __event_call__: Optional[\n            Callable[[Dict[str, Any]], Awaitable[Dict[str, Any]]]\n        ] = None,\n    ) -> Union[Dict[str, Any], StreamingResponse]:\n        \"\"\"\n        Main pipe method supporting both streaming and non-streaming responses\n        \"\"\"\n        try:\n            # Return a streaming response\n            return StreamingResponse(\n                self.stream_response(\n                    body=body,\n                    __request__=__request__,\n                    __user__=__user__,\n                    __event_emitter__=__event_emitter__,\n                    __event_call__=__event_call__,\n                ),\n                media_type=\"text/plain\",\n            )\n\n        except Exception as e:\n            await self.emit_status(__event_emitter__, \"error\", str(e), True)\n            return {\n                \"id\": \"error\",\n                \"model\": \"error\",\n                \"created\": \"2023-10-01T00:00:00Z\",\n                \"choices\": [\n                    {\n                        \"index\": \"1\",\n                        \"message\": {\n                            \"role\": \"assistant\",\n                            \"content\": f\"Error: {e}\",\n                        },\n                    }\n                ],\n            }\n",
    "meta": {
      "description": "Talks to Language Model Gateway",
      "manifest": {
        "title": "LangChain Pipe Function",
        "author": "Colby Sawyer @ Attollo LLC (mailto:colby.sawyer@attollodefense.com)",
        "author_url": "https://github.com/ColbySawyer7",
        "version": "0.1.0"
      }
    },
    "is_active": true,
    "is_global": false,
    "updated_at": 1745989387,
    "created_at": 1745989309
  }
]